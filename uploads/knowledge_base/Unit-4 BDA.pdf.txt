UNIT IV  
Frameworks and Applications: Frameworks: Applications on Big Data Using Pig and Hive, 
Data  processing operators in Pig, Hive services, HiveQL, Querying Data in Hive, 
fundamentals of HBase and  ZooKeeper.  
 
 Apache Pig and Apache Hive are two popular frameworks for processing large -scale data in the 
Hadoop ecosystem.  
 They provide abstraction layers over Hadoop's MapReduce, making it easier to process and analyze 
big data without having to write complex Java -based MapReduce programs.  
1. Apache Pig  
Overview:  
 Pig is a high -level scripting platform that runs on Hadoop.  
 It is designed for data transformation, ETL (Extract, Transform, Load) operations, and iterative data 
processing.  
 It uses Pig Latin , a procedural l anguage that abstracts MapReduce.  
Key Features:  
 Handles structured, semi -structured, and unstructured data . 
 Automatically optimizes execution plans.  
 Supports UDFs (User -Defined Functions)  in Java, Python, and other languages.  
 Works efficiently with large d atasets.  
Use Cases:  
 Data preprocessing and cleaning before storage in a data warehouse.  
 ETL operations for analytics pipelines.  
 Batch processing for log analysis and sentiment analysis.  
2. Apache Hive  
Overview:  
 Hive is a data warehouse system  built on top of Hadoop.  

 It allows users to query large datasets using HiveQL (SQL -like syntax)  instead of writing 
MapReduce programs.  
 Designed for batch processing and analytics.  
Key Features:  
 Provides schema -on-read  (data does not need a predefined schem a). 
 Optimized for aggregations and reporting . 
 Supports ACID transactions  and partitioning for better performance.  
 Integrates with BI tools  like Tableau, Spark, and Tez.  
Use Cases:  
 Business Intelligence and reporting.  
 Ad hoc querying of massive datasets.  
 Data warehousing solutions for structured data.  
Comparison: Pig vs. Hive  
Feature  Apache Pig  Apache Hive  
Language  Pig Latin (Procedural)  HiveQL (Declarative SQL -like) 
Best For  Data transformation (ETL, preprocessing)  Querying and analysis  
Learning Curve  Easier for programmers  Easier for SQL users  
Performance  Good for raw data processing  Optimized for querying large datasets  
Integration  Works with Java, Python, UDFs  Works with BI tools, JDBC/ODBC  
 
Data processing operators in Pig  
Apache Pig provides a rich set of data processing operators in Pig Latin to manipulate and analyze large 
datasets efficiently. These operators can be categorized into different groups based on their functionality:  
1. Load and Store Operators  
These operators are used to read and write data in Pig.  
LOAD  – Loads data from HDFS or other storage into a Pig relation.  
data = LOAD 'hdfs://path/to/file' USING PigStorage(',') AS (id:int, name:chararray, age:int);  
 
 

 
 

 
 

 
 

 
 
Apache Hive: Services, HiveQL, and Querying Data  
1. Hive Services  
Apache Hive provides multiple services that work together to process queries and manage metadata. The 
key Hive services are:  
a. Hive Metastore (HMS)  
 Stores metadata  about Hive tables, schemas, partitions, and data  locations.  
 Can use MySQL, PostgreSQL, or Derby  as the underlying database.  
 Metadata is essential for query optimization and execution.  
b. Hive Driver  
 Acts as the interface between users and Hive . 

 Receives queries , compiles them, optimizes execution plans,  and submits jobs to Hadoop.  
c. Hive Compiler  
 Converts HiveQL queries into MapReduce jobs . 
 Optimizes execution by breaking down complex queries into efficient tasks.  
d. Hive Execution Engine  
 Executes the compiled MapReduce jobs or Tez/Spark queries . 
 Sends jobs to Hadoop for distributed processing.  
e. HiveServer2  
 Enables multi -client connections  via JDBC, ODBC, and Thrift.  
 Supports external applications like BI tools (Tableau, Power BI, etc.) . 
f. Web UI (Beeline & Hue)  
 Beeline : A command -line client for running Hive queries.  
 Hue: A web -based UI for executing queries and visualizing results.  
 
2. HiveQL (Hive Query Language)  
HiveQL is an SQL -like language used in Hive for querying and managing data. It includes:  
a. Data Definition Language (DDL)  
Used to cre ate and manage databases, tables, and partitions.  
 

 
 

 
 
 

 
 

Fundamentals of HBase and ZooKeeper  
1. HBase Fundamentals  
Apache HBase  is a distributed, NoSQL database  that runs on top of Hadoop. It is designed for handling 
large amounts of sparse data  in real time, providing random read/write access  to structured data.  
a. Key Features of HBase  
 Column -Oriented Storage  – Unlike traditional row -based databases, HBase stores data in columns . 
 Schema Flexibility  – It supports dynamic schema changes.  
 Horizont al Scalability  – Can scale by adding more nodes (RegionServers).  
 Strong Consistency  – Ensures data consistency across replicas.  
 Integration with Hadoop  – Works with Hadoop’s HDFS  for storage.  
b. HBase Architecture  
HBase consists of several key components:  
1. HMaster  
 Manages the RegionServers . 
 Handles metadata operations  and load balancing.  
2. RegionServers  
 Store actual data in Regions  (distributed chunks of a table).  
 Handle read/write requests  from clients.  
3. Regions  
 A Region  is a subset of table data stor ed in HFiles  (HDFS).  
 A table is split into multiple Regions  to distribute the load.  
4. HDFS (Hadoop Distributed File System)  
 Stores the actual HBase data files.  
5. ZooKeeper (Coordination Service)  
 Manages leader election, failure recovery, and distributed coordination . 
 Keeps track of the live RegionServers  and HMaster  status.  
c. Data Model in HBase  
HBase is a NoSQL database  that uses a key-value  structure.  
Table Structure  
 Row Key  – The unique identifier for each row.  
 Column Families  – Logical groups of related columns.  
 Columns  – Contain actual data values.  
 Timestamp  – Each cell stores multiple versions of data (based on time).  
Example HBase Table (Employee Data)  
Row Key  Column Family: Personal  Column Family: Work  
101 Name: John  Dept: HR  
 
Age: 30  Location: NY  
102 Name: Alice  Dept: IT  
 
Age: 28  Location: SF  
 
 

2. ZooKeeper Fundamentals  
Apache ZooKeeper  is a distributed coordination service  used to manage synchronization, configuration, 
and failure recovery  in distributed systems like HBase, Hadoop, and Kafka.  
a. Key Features of ZooKeeper  
 Centralized Configuration Management  – Stores system settings for distributed applications.  
 Leader Election  – Helps select the leader node in a cluster.  
 Synchronization  – Ensures consistent data across distributed nodes.  
 Failure Detection and Recovery  – Keeps track of live and dead nodes.  
 High Availability  – Uses replication to avoid single points of failure.  
b. ZooKeeper Architecture  
ZooKeeper follows a client -server mo del with three key components:  
1. ZooKeeper Ensemble (Cluster)  
 Comprises multiple ZooKeeper nodes ( Leader + Followers ). 
 Uses replicated consensus (ZAB protocol)  to maintain consistency.  
2. ZNodes (ZooKeeper Nodes)  
 Data units in ZooKeeper , similar to files in a filesystem.  
 Supports ephemeral nodes  (temporary) and persistent nodes . 
3. Clients (HBase, Hadoop, Kafka, etc.)  
 Applications that connect to ZooKeeper to retrieve configuration data or participate in distributed 
coordination.  
 

 
3. How HBase Uses ZooKeeper  
HBase depends on ZooKeeper  for: 
 Tracking Active HMaster  – Ensures only one HMaster is active at a time.  
 RegionServer Management  – Keeps a list of live RegionServers . 
 Failure Recovery  – Automatically detects and reassigns Regions  in case of server  failures.  
 
